
---
title: "GAMWOOD"
author: "Alix Rigal"

date: "`r Sys.Date()`"
output: 
 pdf_document: 
   toc: yes
   number_sections: yes
   keep_tex: yes
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Une première lecture de WOOD 2016

## package utilisé
-gamair : Data sets and scripts used in the book 'Generalized Additive Models: An Introduction with R
-nlme: Fit and compare Gaussian linear and nonlinear mixed-effects models.


#Generalised additive model an introduction S Wood
#debut au chap 2 LMM

```{r}

require(gamair)

data(stomata)

summary(stomata)

m1 <- lm(area ~ CO2 + tree, stomata)
summary(m1)

m0 <- lm(area ~ CO2, stomata)
summary(m0)
anova(m0,m1)

m2 <- lm(area ~ tree, stomata)
anova(m2,m1)
```


conclusion : on ne peut pas conclure à un effet du CO2 ou des arbres qui soit significatif 
etant donné que les données sont dépendantes de l'arbre en question
seul le model contenant les deux effet donne de bon resultat selon fisher 

## test model mixte

```{r}
st <- aggregate(data.matrix(stomata), by=list(tree=stomata$tree),mean)#Matrice contenant la moyenne pour chaque arbre
st$CO2 <- as.factor(st$CO2);st
m3 <- lm(area ~ CO2, st)

summary(m3)
summary(m3)$sigma^2 - summary(m1)$sigma^2/4
```

```{r}
library(nlme) # load nlme ‘library’, which contains data
data(Rail) # load data
summary(Rail)
m1 <- lm(travel ~ Rail,Rail)
 anova(m1)
 rt <- aggregate(data.matrix(Rail),by=list(Rail$Rail),mean) # average over Rail effect
 rt
 
 m0 <- lm(travel ~ 1,rt) # fit model to aggregated data
 sig <- summary(m1)$sigma # sig^2 is resid. var. component
  sigb <- (summary(m0)$sigma^2 - sig^2/3)^0.5
  # sigb^2 is the variance component for rail
  sigb
  sig
  
```

```{r}
 library(nlme)
  data(Machines)
  names(Machines)

  attach(Machines) # make data available without ‘Machines$’ ie colones of the data frame are now available in the global environment
  interaction.plot(Machine,Worker,score)
  m1 <- lm(score ~ Worker*Machine,Machines)#interaction + main effect
  m0 <- lm(score ~ Worker + Machine,Machines)
  anova(m0,m1)
  summary(m1)$sigma^2
  Mach <- aggregate(data.matrix(Machines),by=
                      list(Machines$Worker,Machines$Machine),mean)
  Mach$Worker <- as.factor(Mach$Worker)
  Mach$Machine <- as.factor(Mach$Machine)
  m0 <- lm(score ~ Worker + Machine,Mach)
  anova(m0) 
  


```

##Maximizing the profile likelihood

function to evaluate the negative log profile likelihood for the rail data i.e.
$\Lambda_\theta=\sigma I_n$ $\Phi_\theta=\sigma_b I_p$

```{r}
  #input: 
#theta:variance components, X:design matrix fixed effects, Z:design matrix random effects y:dependent variable
  #output: log profile likelihoo, \hat beta and \hat b
  
  llm <- function(theta,X,Z,y) {
    ## untransform parameters...log parametrisation ensures positivity
    sigma.b <- exp(theta[1])
    sigma <- exp(theta[2])
    ## extract dimensions...
    n <- length(y); pr <- ncol(Z); pf <- ncol(X)
    ## obtain \hat \beta, \hat b...
    X1 <- cbind(X,Z)
    ipsi <- c(rep(0,pf),rep(1/sigma.b^2,pr))
    b1 <- solve(crossprod(X1)/sigma^2+diag(ipsi),t(X1)%*%y/sigma^2)
    ## compute log det ⁼log|  Z’Z/sigma^2 + I/sigma.b^2  |...
    ldet <- sum(log(diag(chol(crossprod(Z)/sigma^2 + diag(ipsi[-(1:pf)])))))
    ## compute log profile likelihood...
    l <- (-sum((y-X1%*%b1)^2)/sigma^2 - sum(b1^2*ipsi) -
            n*log(sigma^2) - pr*log(sigma.b^2) - 2*ldet - n*log(2*pi))/2
    attr(l,"b") <- as.numeric(b1) ## return \hat beta and \hat b
    -l
  }
  
## Back to the rail data set

options(contrasts=c("contr.treatment","contr.treatment"))
#The first argument handles unordered categorical variables, the second, ordered.
 Z <- model.matrix(~ Rail$Rail - 1) ## r.e. model matrix
 X <- matrix(1,18,1) ## fixed model matrix
## fit the model...
 rail.mod <- optim(c(0,0),llm,hessian=TRUE,X=X,Z=Z,y=Rail$travel)
exp(rail.mod$par) ## variance components
solve(rail.mod$hessian) ## approx cov matrix for theta

```
The "constrasts" set in your R environment determine how categorical variables are handled in your models.

## to do
For computational efficiency:
nlme and lme4 for. The former is designed to exploit the sparsity that
arises when models have a nested structure, while the latter uses sparse direct
matrix methods (e.g., Davis, 2006) to exploit any sparsity pattern.
